{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predição de Temperatura com CNN 1D - TinyML\n",
        "\n",
        "**Projeto:** Sistema de Predição de Temperatura para Microcontroladores\n",
        "\n",
        "**Objetivo:** Prever a temperatura do sensor AHT20 em 5, 10 e 15 minutos no futuro\n",
        "\n",
        "**Sensores utilizados:**\n",
        "- AHT20: Temperatura e Umidade\n",
        "- BMP280: Temperatura e Pressão\n",
        "\n",
        "**Modelo:** CNN 1D (Convolutional Neural Network 1D) compatível com TensorFlow Lite Micro\n",
        "\n",
        "**Deploy:** Raspberry Pi Pico (RP2040)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Carregar os Dados\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow e Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Sklearn para pré-processamento e métricas\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Configuração de visualização\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "# Configurar seed para reprodutibilidade\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f'TensorFlow: {tf.__version__}')\n",
        "print(f'Seed configurado: {SEED}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar o dataset de temperatura\n",
        "df = pd.read_csv('data/temp.csv')\n",
        "\n",
        "# Converter coluna de timestamp para datetime\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "print(f'Dataset carregado com sucesso!')\n",
        "print(f'Shape: {df.shape}')\n",
        "print(f'Período dos dados: {df[\"Timestamp\"].min()} até {df[\"Timestamp\"].max()}')\n",
        "print(f'\\nPrimeiras linhas:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informações sobre as colunas do dataset\n",
        "print('Informações do dataset:')\n",
        "print(df.info())\n",
        "print('\\n' + '='*70)\n",
        "print('Estatísticas descritivas:')\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Análise Exploratória de Dados (EDA)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar valores nulos\n",
        "print('Valores nulos por coluna:')\n",
        "print(df.isna().sum())\n",
        "print(f'\\nTotal de valores nulos: {df.isna().sum().sum()}')\n",
        "\n",
        "# Se houver nulos, remover\n",
        "if df.isna().sum().sum() > 0:\n",
        "    print(f'\\nRemovendo {df.isna().sum().sum()} linhas com valores nulos...')\n",
        "    df = df.dropna()\n",
        "    print(f'Novo shape: {df.shape}')\n",
        "else:\n",
        "    print('\\nNenhum valor nulo encontrado! Dataset limpo.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular frequência de amostragem\n",
        "print('Análise da frequência de amostragem:')\n",
        "print('='*70)\n",
        "\n",
        "time_diffs = df['Timestamp'].diff()\n",
        "print('\\nDiferenças de tempo entre amostras consecutivas:')\n",
        "print(time_diffs.value_counts().head(10))\n",
        "\n",
        "avg_interval = time_diffs.median()\n",
        "print(f'\\nIntervalo mediano: {avg_interval}')\n",
        "print(f'Frequência de amostragem: {avg_interval.total_seconds():.1f} segundos')\n",
        "print(f'Isso equivale a: {60/avg_interval.total_seconds():.2f} amostras por minuto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Visualização das Séries Temporais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar pasta images/Conv1D se não existir\n",
        "import os\n",
        "os.makedirs('images/Conv1D', exist_ok=True)\n",
        "\n",
        "# Visualizar as séries temporais dos sensores\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# AHT20: Temperatura\n",
        "axes[0, 0].plot(df['Timestamp'], df['Temp_AHT20_C'], linewidth=0.8, alpha=0.7, color='orange')\n",
        "axes[0, 0].set_ylabel('Temperatura (°C)', fontsize=12)\n",
        "axes[0, 0].set_title('AHT20: Temperatura', fontweight='bold', fontsize=14)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# AHT20: Umidade\n",
        "axes[0, 1].plot(df['Timestamp'], df['Umid_AHT20_pct'], linewidth=0.8, alpha=0.7, color='green')\n",
        "axes[0, 1].set_ylabel('Umidade (%)', fontsize=12)\n",
        "axes[0, 1].set_title('AHT20: Umidade', fontweight='bold', fontsize=14)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# BMP280: Temperatura\n",
        "axes[1, 0].plot(df['Timestamp'], df['Temp_BMP280_C'], linewidth=0.8, alpha=0.7, color='red')\n",
        "axes[1, 0].set_ylabel('Temperatura (°C)', fontsize=12)\n",
        "axes[1, 0].set_xlabel('Tempo', fontsize=12)\n",
        "axes[1, 0].set_title('BMP280: Temperatura', fontweight='bold', fontsize=14)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# BMP280: Pressão\n",
        "axes[1, 1].plot(df['Timestamp'], df['Press_BMP280_hPa'], linewidth=0.8, alpha=0.7, color='purple')\n",
        "axes[1, 1].set_ylabel('Pressão (hPa)', fontsize=12)\n",
        "axes[1, 1].set_xlabel('Tempo', fontsize=12)\n",
        "axes[1, 1].set_title('BMP280: Pressão', fontweight='bold', fontsize=14)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/01_series_temporais.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/01_series_temporais.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Distribuição das Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar distribuição das features principais\n",
        "features_to_plot = ['Temp_AHT20_C', 'Umid_AHT20_pct', 'Temp_BMP280_C', 'Press_BMP280_hPa']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    axes[idx].hist(df[feature], bins=50, alpha=0.7, edgecolor='black')\n",
        "    axes[idx].axvline(df[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'Média: {df[feature].mean():.2f}')\n",
        "    axes[idx].axvline(df[feature].median(), color='green', linestyle='--', linewidth=2, label=f'Mediana: {df[feature].median():.2f}')\n",
        "    axes[idx].set_xlabel(feature, fontsize=12)\n",
        "    axes[idx].set_ylabel('Frequência', fontsize=12)\n",
        "    axes[idx].set_title(f'Distribuição: {feature}', fontweight='bold', fontsize=14)\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/02_distribuicao_features.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/02_distribuicao_features.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3. Matriz de Correlação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular matriz de correlação das features numéricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Visualizar matriz de correlação\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Matriz de Correlação entre Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/03_matriz_correlacao.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/03_matriz_correlacao.png')\n",
        "print('\\nCorrelações mais fortes com Temp_AHT20_C (target):')\n",
        "target_corr = correlation_matrix['Temp_AHT20_C'].sort_values(ascending=False)\n",
        "print(target_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4. Estatísticas Resumidas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estatísticas resumidas das features principais\n",
        "print('='*70)\n",
        "print('ESTATÍSTICAS RESUMIDAS')\n",
        "print('='*70)\n",
        "\n",
        "for feature in features_to_plot:\n",
        "    print(f'\\n{feature}:')\n",
        "    print(f'  Média:    {df[feature].mean():.4f}')\n",
        "    print(f'  Mediana:  {df[feature].median():.4f}')\n",
        "    print(f'  Desvio:   {df[feature].std():.4f}')\n",
        "    print(f'  Mínimo:   {df[feature].min():.4f}')\n",
        "    print(f'  Máximo:   {df[feature].max():.4f}')\n",
        "    print(f'  Variação: {df[feature].max() - df[feature].min():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Pré-processamento dos Dados\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1. Remoção de Outliers\n",
        "\n",
        "Utilizamos o método IQR (Interquartile Range) para identificar e remover outliers.\n",
        "\n",
        "**Fórmula:**\n",
        "- Quartil 1 (Q1) = 25º percentil\n",
        "- Quartil 3 (Q3) = 75º percentil\n",
        "- IQR = Q3 - Q1\n",
        "- Limite inferior = Q1 - 1.5 × IQR\n",
        "- Limite superior = Q3 + 1.5 × IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar remoção de outliers usando IQR\n",
        "print('Remoção de outliers:')\n",
        "print(f'Antes: {len(df)} amostras')\n",
        "\n",
        "df_clean = df.copy()\n",
        "columns_to_clean = ['Temp_AHT20_C', 'Umid_AHT20_pct', 'Temp_BMP280_C', 'Press_BMP280_hPa']\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    Q1 = df_clean[col].quantile(0.25)\n",
        "    Q3 = df_clean[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
        "\n",
        "print(f'Depois: {len(df_clean)} amostras')\n",
        "removed = len(df) - len(df_clean)\n",
        "print(f'Removidos: {removed} ({100*removed/len(df):.2f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2. Criar Sequências Temporais\n",
        "\n",
        "**Conceito:** Transformar dados tabulares em sequências 3D para CNN 1D\n",
        "\n",
        "**Formato:**\n",
        "- Input (X): Janela de 10 timesteps com 4 features cada\n",
        "- Output (y): Temperatura futura em 3 horizontes (5, 10, 15 minutos)\n",
        "\n",
        "**Exemplo:**\n",
        "```\n",
        "X = [timestep_0, timestep_1, ..., timestep_9]  <- 10 timesteps passados\n",
        "y = [temp_t+10, temp_t+19, temp_t+29]           <- 3 previsões futuras\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar parâmetros das sequências\n",
        "WINDOW_SIZE = 10\n",
        "HORIZONS = [10, 19, 29]\n",
        "\n",
        "print('Configuração das sequências:')\n",
        "print(f'  WINDOW_SIZE = {WINDOW_SIZE} timesteps')\n",
        "print(f'  HORIZONS = {HORIZONS}')\n",
        "print(f'\\nIsso significa:')\n",
        "print(f'  - Observar últimos {WINDOW_SIZE * 31 / 60:.1f} minutos')\n",
        "print(f'  - Prever temperatura em {HORIZONS[0] * 31 / 60:.1f}, {HORIZONS[1] * 31 / 60:.1f} e {HORIZONS[2] * 31 / 60:.1f} minutos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(data, window_size, horizons):\n",
        "    \"\"\"\n",
        "    Criar sequências temporais para treinamento\n",
        "    \n",
        "    Parâmetros:\n",
        "    - data: DataFrame com os dados\n",
        "    - window_size: tamanho da janela de observação\n",
        "    - horizons: lista de horizontes de previsão\n",
        "    \n",
        "    Retorna:\n",
        "    - X: array 3D (samples, timesteps, features)\n",
        "    - y: array 2D (samples, horizons)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    features = ['Temp_AHT20_C', 'Umid_AHT20_pct', 'Temp_BMP280_C', 'Press_BMP280_hPa']\n",
        "    target = 'Temp_AHT20_C'\n",
        "    \n",
        "    for i in range(len(data) - window_size - max(horizons)):\n",
        "        X.append(data.iloc[i:i+window_size][features].values)\n",
        "        y.append([data.iloc[i+window_size+h][target] for h in horizons])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Criar sequências\n",
        "X, y = create_sequences(df_clean, WINDOW_SIZE, HORIZONS)\n",
        "\n",
        "print('Sequências criadas com sucesso!')\n",
        "print(f'\\nX shape: {X.shape}')\n",
        "print(f'  - {X.shape[0]} amostras')\n",
        "print(f'  - {X.shape[1]} timesteps por amostra')\n",
        "print(f'  - {X.shape[2]} features por timestep')\n",
        "print(f'\\ny shape: {y.shape}')\n",
        "print(f'  - {y.shape[0]} amostras')\n",
        "print(f'  - {y.shape[1]} horizontes de previsão')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3. Divisão em Treino, Validação e Teste\n",
        "\n",
        "**Divisão:**\n",
        "- Treino: 70% dos dados (para treinar o modelo)\n",
        "- Validação: 15% dos dados (para ajustar hiperparâmetros)\n",
        "- Teste: 15% dos dados (para avaliar desempenho final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir dados em treino, validação e teste\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size = int(0.85 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:val_size], y[train_size:val_size]\n",
        "X_test, y_test = X[val_size:], y[val_size:]\n",
        "\n",
        "print('Dados divididos:')\n",
        "print(f'  Treino:    {len(X_train):6d} amostras ({100*len(X_train)/len(X):.1f}%)')\n",
        "print(f'  Validação: {len(X_val):6d} amostras ({100*len(X_val)/len(X):.1f}%)')\n",
        "print(f'  Teste:     {len(X_test):6d} amostras ({100*len(X_test)/len(X):.1f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4. Normalização dos Dados\n",
        "\n",
        "**Por que normalizar?**\n",
        "- Features têm escalas diferentes (temperatura ~20°C, pressão ~920 hPa)\n",
        "- Redes neurais aprendem melhor com dados normalizados\n",
        "- Evita que features com valores grandes dominem o treinamento\n",
        "\n",
        "**Método:** StandardScaler (Z-score normalization)\n",
        "- Transforma dados para média = 0 e desvio padrão = 1\n",
        "- Fórmula: (x - média) / desvio_padrão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizar dados usando StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
        "scaler.fit(X_train_reshaped)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
        "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "print('Dados normalizados com sucesso!')\n",
        "print(f'\\nParâmetros do scaler:')\n",
        "print(f'  Média (treino): {X_train_scaled.mean():.6f}')\n",
        "print(f'  Desvio (treino): {X_train_scaled.std():.6f}')\n",
        "print(f'\\nMédias por feature:')\n",
        "feature_names = ['Temp_AHT20_C', 'Umid_AHT20_pct', 'Temp_BMP280_C', 'Press_BMP280_hPa']\n",
        "for i, name in enumerate(feature_names):\n",
        "    print(f'  {name:20s}: mean={scaler.mean_[i]:8.4f}, std={scaler.scale_[i]:8.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Construção e Treinamento do Modelo\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1. Arquitetura do Modelo CNN 1D\n",
        "\n",
        "**Por que CNN 1D?**\n",
        "- Captura padrões temporais em dados sequenciais\n",
        "- Mais leve que LSTM/GRU\n",
        "- Compatível com TensorFlow Lite Micro\n",
        "\n",
        "**Arquitetura:**\n",
        "```\n",
        "Input (10 timesteps × 4 features)\n",
        "    |\n",
        "Conv1D(24 filtros, kernel=3) + ReLU + Dropout(0.2)\n",
        "    |\n",
        "Conv1D(16 filtros, kernel=3) + ReLU\n",
        "    |\n",
        "GlobalAveragePooling1D\n",
        "    |\n",
        "Dense(24) + ReLU + Dropout(0.2)\n",
        "    |\n",
        "Dense(3) [Output: 3 previsões]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construir modelo CNN 1D\n",
        "model = Sequential([\n",
        "    Conv1D(24, kernel_size=3, activation='relu', \n",
        "           kernel_regularizer=regularizers.l2(0.0001),\n",
        "           input_shape=(WINDOW_SIZE, 4),\n",
        "           name='conv1d_1'),\n",
        "    Dropout(0.2, name='dropout_1'),\n",
        "    Conv1D(16, kernel_size=3, activation='relu',\n",
        "           kernel_regularizer=regularizers.l2(0.0001),\n",
        "           name='conv1d_2'),\n",
        "    GlobalAveragePooling1D(name='global_avg_pool'),\n",
        "    Dense(24, activation='relu', \n",
        "          kernel_regularizer=regularizers.l2(0.0001),\n",
        "          name='dense_1'),\n",
        "    Dropout(0.2, name='dropout_2'),\n",
        "    Dense(3, activation='linear', name='output')\n",
        "], name='CNN1D_Temperature')\n",
        "\n",
        "# Compilar modelo\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print('Modelo criado com sucesso!')\n",
        "print('\\nArquitetura:')\n",
        "model.summary()\n",
        "\n",
        "print(f'\\nTotal de parâmetros: {model.count_params():,}')\n",
        "print(f'Tamanho estimado: {model.count_params() * 4 / 1024:.2f} KB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2. Configurar Callbacks\n",
        "\n",
        "**Callbacks** são funções que executam durante o treinamento:\n",
        "\n",
        "1. **EarlyStopping**: Para o treinamento se a validação não melhorar\n",
        "2. **ReduceLROnPlateau**: Reduz learning rate quando estagnar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=50,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=20,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print('Callbacks configurados:')\n",
        "print('  - EarlyStopping (patience=50)')\n",
        "print('  - ReduceLROnPlateau (factor=0.5, patience=20)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3. Treinar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treinar modelo\n",
        "print('Iniciando treinamento...')\n",
        "print('='*70)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=300,\n",
        "    batch_size=512,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('Treinamento concluído!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4. Visualizar Curvas de Aprendizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar curvas de loss e MAE\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validação', linewidth=2)\n",
        "axes[0].set_xlabel('Época', fontsize=12)\n",
        "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
        "axes[0].set_title('Curva de Loss', fontweight='bold', fontsize=14)\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history['mae'], label='Treino', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validação', linewidth=2)\n",
        "axes[1].set_xlabel('Época', fontsize=12)\n",
        "axes[1].set_ylabel('MAE (°C)', fontsize=12)\n",
        "axes[1].set_title('Curva de MAE', fontweight='bold', fontsize=14)\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/04_curvas_aprendizado.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/04_curvas_aprendizado.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Avaliação do Modelo\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1. Métricas no Conjunto de Teste\n",
        "\n",
        "**Métricas utilizadas:**\n",
        "- **MAE** (Mean Absolute Error): Erro médio absoluto em °C\n",
        "- **RMSE** (Root Mean Squared Error): Raiz do erro quadrático médio\n",
        "- **R²** (Coefficient of Determination): Quanto o modelo explica a variância"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fazer predições no conjunto de teste\n",
        "y_pred = model.predict(X_test_scaled, verbose=0)\n",
        "\n",
        "# Calcular métricas gerais\n",
        "mae_overall = mean_absolute_error(y_test, y_pred)\n",
        "rmse_overall = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2_overall = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calcular MAE por horizonte\n",
        "mae_t1 = mean_absolute_error(y_test[:, 0], y_pred[:, 0])\n",
        "mae_t2 = mean_absolute_error(y_test[:, 1], y_pred[:, 1])\n",
        "mae_t3 = mean_absolute_error(y_test[:, 2], y_pred[:, 2])\n",
        "\n",
        "print('='*70)\n",
        "print('RESULTADOS NO CONJUNTO DE TESTE')\n",
        "print('='*70)\n",
        "print(f'\\nMétricas Gerais:')\n",
        "print(f'  MAE:  {mae_overall:.4f} °C')\n",
        "print(f'  RMSE: {rmse_overall:.4f} °C')\n",
        "print(f'  R²:   {r2_overall:.4f}')\n",
        "print(f'\\nMAE por Horizonte:')\n",
        "print(f'  T+10 (5 min):   {mae_t1:.4f} °C')\n",
        "print(f'  T+19 (10 min):  {mae_t2:.4f} °C')\n",
        "print(f'  T+29 (15 min):  {mae_t3:.4f} °C')\n",
        "print(f'\\nInformações do Modelo:')\n",
        "print(f'  Parâmetros totais: {model.count_params():,}')\n",
        "print(f'  Tamanho estimado: {model.count_params() * 4 / 1024:.2f} KB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2. Gráficos de Predito vs Real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar scatter plots para cada horizonte\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "horizons_names = ['T+10 (5 min)', 'T+19 (10 min)', 'T+29 (15 min)']\n",
        "\n",
        "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
        "    ax.scatter(y_test[:, i], y_pred[:, i], alpha=0.4, s=15)\n",
        "    min_val = min(y_test[:, i].min(), y_pred[:, i].min())\n",
        "    max_val = max(y_test[:, i].max(), y_pred[:, i].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideal')\n",
        "    ax.set_xlabel('Real (°C)', fontsize=12)\n",
        "    ax.set_ylabel('Predito (°C)', fontsize=12)\n",
        "    ax.set_title(horizon_name, fontweight='bold', fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
        "    r2 = r2_score(y_test[:, i], y_pred[:, i])\n",
        "    ax.text(0.05, 0.95, f'MAE: {mae:.3f}°C\\nR²: {r2:.3f}',\n",
        "            transform=ax.transAxes, va='top', fontsize=11,\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/05_scatter_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/05_scatter_predictions.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3. Séries Temporais de Predições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar série temporal das predições\n",
        "n_samples = 300\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
        "    ax.plot(y_test[:n_samples, i], label='Real', linewidth=2, alpha=0.8)\n",
        "    ax.plot(y_pred[:n_samples, i], label='Predito', linewidth=2, alpha=0.8)\n",
        "    ax.set_ylabel('Temperatura (°C)', fontsize=12)\n",
        "    ax.set_title(horizon_name, fontweight='bold', fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].set_xlabel('Amostra', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/06_timeseries_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/06_timeseries_predictions.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4. Análise de Erros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular erros\n",
        "errors = y_pred - y_test\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
        "    ax.hist(errors[:, i], bins=50, alpha=0.7, edgecolor='black')\n",
        "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Erro zero')\n",
        "    ax.set_xlabel('Erro (°C)', fontsize=12)\n",
        "    ax.set_ylabel('Frequência', fontsize=12)\n",
        "    ax.set_title(f'{horizon_name}: Distribuição dos Erros', fontweight='bold', fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    mean_err = errors[:, i].mean()\n",
        "    std_err = errors[:, i].std()\n",
        "    ax.text(0.05, 0.95, f'Média: {mean_err:.3f}\\nStd: {std_err:.3f}',\n",
        "            transform=ax.transAxes, va='top', fontsize=11,\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('images/Conv1D/07_error_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('Gráfico salvo: images/Conv1D/07_error_distribution.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Conversão para TensorFlow Lite\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1. Salvar Modelo Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar pasta models se não existir\n",
        "os.makedirs('models/Conv1D', exist_ok=True)\n",
        "\n",
        "model.save('models/Conv1D/temperature_model.keras')\n",
        "print('Modelo Keras salvo: models/Conv1D/temperature_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2. Converter para TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converter para TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('models/Conv1D/temperature_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "size_kb = len(tflite_model) / 1024\n",
        "print(f'Modelo TFLite salvo: models/Conv1D/temperature_model.tflite')\n",
        "print(f'Tamanho: {size_kb:.2f} KB')\n",
        "\n",
        "if size_kb < 100:\n",
        "    print('Compatível com Raspberry Pi Pico (2MB Flash)')\n",
        "else:\n",
        "    print('AVISO: Modelo grande para microcontrolador')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3. Verificar Conversão TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testar modelo TFLite\n",
        "interpreter = tf.lite.Interpreter(model_path='models/Conv1D/temperature_model.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print('Informações do modelo TFLite:')\n",
        "print(f'  Input shape:  {input_details[0][\"shape\"]}')\n",
        "print(f'  Input dtype:  {input_details[0][\"dtype\"]}')\n",
        "print(f'  Output shape: {output_details[0][\"shape\"]}')\n",
        "print(f'  Output dtype: {output_details[0][\"dtype\"]}')\n",
        "\n",
        "test_sample = X_test_scaled[0:1].astype(np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
        "interpreter.invoke()\n",
        "tflite_pred = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "keras_pred = model.predict(test_sample, verbose=0)\n",
        "\n",
        "print(f'\\nTeste de conversão:')\n",
        "print(f'  Predição Keras:  {keras_pred[0]}')\n",
        "print(f'  Predição TFLite: {tflite_pred[0]}')\n",
        "print(f'  Diferença máxima: {np.abs(keras_pred - tflite_pred).max():.6f} °C')\n",
        "\n",
        "if np.abs(keras_pred - tflite_pred).max() < 0.01:\n",
        "    print('\\nConversão TFLite: OK')\n",
        "else:\n",
        "    print('\\nAVISO: Diferença significativa após conversão')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Geração de Arquivo C para RP2040\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1. Converter Modelo TFLite para C Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_c_array(tflite_model, var_name='model_data'):\n",
        "    c_array = f\"const unsigned char {var_name}[] = {{\\n\"\n",
        "    for i in range(0, len(tflite_model), 12):\n",
        "        line = \"  \"\n",
        "        for j in range(12):\n",
        "            if i + j < len(tflite_model):\n",
        "                line += f\"0x{tflite_model[i+j]:02x}, \"\n",
        "        c_array += line + \"\\n\"\n",
        "    c_array += \"};\\n\"\n",
        "    c_array += f\"const unsigned int {var_name}_len = {len(tflite_model)};\\n\"\n",
        "    return c_array\n",
        "\n",
        "os.makedirs('models/Conv1D', exist_ok=True)\n",
        "\n",
        "h_content = \"\"\"// Temperature Prediction Model - TinyML\n",
        "// Auto-generated file - Do not edit manually\n",
        "// Model trained on AHT20 + BMP280 sensor data\n",
        "\n",
        "#ifndef TEMPERATURE_MODEL_H\n",
        "#define TEMPERATURE_MODEL_H\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "h_content += convert_to_c_array(tflite_model, 'temperature_model')\n",
        "\n",
        "h_content += \"\"\"\n",
        "// Model information\n",
        "#define WINDOW_SIZE 10\n",
        "#define NUM_FEATURES 4\n",
        "#define NUM_HORIZONS 3\n",
        "\n",
        "// Feature names\n",
        "const char* feature_names[] = {\n",
        "    \"Temp_AHT20_C\",\n",
        "    \"Umid_AHT20_pct\",\n",
        "    \"Temp_BMP280_C\",\n",
        "    \"Press_BMP280_hPa\"\n",
        "};\n",
        "\n",
        "// Horizon names\n",
        "const char* horizon_names[] = {\n",
        "    \"5 minutes\",\n",
        "    \"10 minutes\",\n",
        "    \"15 minutes\"\n",
        "};\n",
        "\n",
        "#endif // TEMPERATURE_MODEL_H\n",
        "\"\"\"\n",
        "\n",
        "h_filename = 'models/Conv1D/temperature_model.h'\n",
        "with open(h_filename, 'w') as f:\n",
        "    f.write(h_content)\n",
        "\n",
        "print(f'Arquivo {h_filename} gerado com sucesso!')\n",
        "print(f'Tamanho do modelo: {len(tflite_model)} bytes ({len(tflite_model)/1024:.2f} KB)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2. Gerar Parâmetros do Scaler para C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerar arquivo header com parâmetros de normalização\n",
        "scaler_h_content = \"\"\"// Scaler parameters for normalization\n",
        "// Auto-generated file - Do not edit manually\n",
        "\n",
        "#ifndef SCALER_PARAMS_H\n",
        "#define SCALER_PARAMS_H\n",
        "\n",
        "// Mean values\n",
        "const float scaler_mean[] = {\n",
        "\"\"\"\n",
        "\n",
        "for i, mean_val in enumerate(scaler.mean_):\n",
        "    scaler_h_content += f\"    {mean_val:.6f}f\"\n",
        "    if i < len(scaler.mean_) - 1:\n",
        "        scaler_h_content += \",\"\n",
        "    scaler_h_content += f\"  // {feature_names[i]}\\n\"\n",
        "\n",
        "scaler_h_content += \"\"\"};\\n\n",
        "// Scale values\n",
        "const float scaler_scale[] = {\n",
        "\"\"\"\n",
        "\n",
        "for i, scale_val in enumerate(scaler.scale_):\n",
        "    scaler_h_content += f\"    {scale_val:.6f}f\"\n",
        "    if i < len(scaler.scale_) - 1:\n",
        "        scaler_h_content += \",\"\n",
        "    scaler_h_content += f\"  // {feature_names[i]}\\n\"\n",
        "\n",
        "scaler_h_content += \"\"\"};\\n\n",
        "#endif // SCALER_PARAMS_H\n",
        "\"\"\"\n",
        "\n",
        "scaler_filename = 'models/Conv1D/scaler_params.h'\n",
        "with open(scaler_filename, 'w') as f:\n",
        "    f.write(scaler_h_content)\n",
        "\n",
        "print(f'Arquivo {scaler_filename} gerado com sucesso!')\n",
        "print(f'\\nParâmetros do Scaler:')\n",
        "print(f'  Mean: {scaler.mean_}')\n",
        "print(f'  Scale: {scaler.scale_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3. Salvar Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(scaler, 'models/Conv1D/scaler.pkl')\n",
        "print('Scaler salvo: models/Conv1D/scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Resumo Final\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('RESUMO DO PROJETO')\n",
        "print('='*70)\n",
        "\n",
        "print(f'\\nDataset:')\n",
        "print(f'  - Total amostras: {len(df_clean):,}')\n",
        "print(f'  - Features: {feature_names}')\n",
        "print(f'  - Target: Temp_AHT20_C')\n",
        "print(f'  - Período: {df_clean[\"Timestamp\"].min()} até {df_clean[\"Timestamp\"].max()}')\n",
        "\n",
        "print(f'\\nModelo:')\n",
        "print(f'  - Arquitetura: CNN 1D (2 camadas Conv + Dense)')\n",
        "print(f'  - Parâmetros: {model.count_params():,}')\n",
        "print(f'  - Tamanho TFLite: {len(tflite_model)/1024:.2f} KB')\n",
        "print(f'  - Compatível: TensorFlow Lite Micro')\n",
        "\n",
        "print(f'\\nPerformance (Teste):')\n",
        "print(f'  - MAE Geral:    {mae_overall:.4f} °C')\n",
        "print(f'  - RMSE:         {rmse_overall:.4f} °C')\n",
        "print(f'  - R²:           {r2_overall:.4f}')\n",
        "print(f'  - MAE (5 min):  {mae_t1:.4f} °C')\n",
        "print(f'  - MAE (10 min): {mae_t2:.4f} °C')\n",
        "print(f'  - MAE (15 min): {mae_t3:.4f} °C')\n",
        "\n",
        "print(f'\\nArquivos Gerados:')\n",
        "print(f'  - models/Conv1D/temperature_model.keras')\n",
        "print(f'  - models/Conv1D/temperature_model.tflite')\n",
        "print(f'  - models/Conv1D/temperature_model.h')\n",
        "print(f'  - models/Conv1D/scaler_params.h')\n",
        "print(f'  - models/Conv1D/scaler.pkl')\n",
        "print(f'  - images/Conv1D/01-07.png')\n",
        "\n",
        "print(f'\\nPróximos Passos:')\n",
        "print(f'  1. Integrar models/Conv1D/*.h no código do RP2040')\n",
        "print(f'  2. Implementar loop de leitura dos sensores')\n",
        "print(f'  3. Aplicar normalização usando scaler_params.h')\n",
        "print(f'  4. Executar inferência com TFLite Micro')\n",
        "print(f'  5. Exibir predições de temperatura')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('PROJETO CONCLUÍDO COM SUCESSO!')\n",
        "print('='*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}